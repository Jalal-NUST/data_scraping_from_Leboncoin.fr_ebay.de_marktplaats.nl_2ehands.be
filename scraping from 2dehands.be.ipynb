{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,time,random,queue\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "data_new= pd.DataFrame(columns= ['Title','Price','item_link'])\n",
    "data_new1= pd.DataFrame(columns= ['Title','Price','Description','item_link'])\n",
    "q = queue.Queue()\n",
    "def clear(): \n",
    "  \n",
    "    # for windows \n",
    "    if name == 'nt': \n",
    "        _ = system('cls') \n",
    "  \n",
    "    # for mac and linux(here, os.name is 'posix') \n",
    "    else: \n",
    "        _ = system('clear') \n",
    "chrome_options = Options()\n",
    "\n",
    "############################################################\n",
    "class Scrap:\n",
    "    \n",
    "\n",
    "    def popup(self):\n",
    "        self.browser = webdriver.Chrome('chromedriver.exe', options=chrome_options)\n",
    "        self.browser.get('https://www.2dehands.be/')\n",
    "        time.sleep(10)\n",
    "        self.browser.find_element_by_id(\"gdpr-consent-banner-accept-button\").click()\n",
    "        \n",
    "    def search(self,search_item='porselein'):\n",
    "        search_200 = 'https://www.2dehands.be/q/{s}/'.format(s=search_item)\n",
    "        self.browser.get(search_200)\n",
    "        time.sleep(2)\n",
    "    def next_page(self,page_no,search_item='porselein'):\n",
    "        next_page ='https://www.2dehands.be/q/{s}/p/{p}/'.format(s=search_item,p=page_no)\n",
    "        self.browser.get(next_page)\n",
    "        try:\n",
    "            browser.find_element_by_class_name('hz-Icon hz-Icon--m hz-SvgIcon hz-SvgIconClose').click()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "    def scrap_data(self):\n",
    "        dic_new={}\n",
    "        src = self.browser.page_source\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        total_pages= soup.find('span',{'class':\"mp-PaginationControls-pagination-pageList\"}).find_all('span')[2].get_text()\n",
    "        print('Total ',total_pages,' pages found...')\n",
    "        \n",
    "        items = soup.find_all('li',{'class':\"mp-Listing mp-Listing--list-item \"})\n",
    "        time.sleep(3)\n",
    "        for item in range(33):\n",
    "            dic_new={}\n",
    "            title = items[item].find('h3',{'class':\"mp-Listing-title\"}).get_text().strip()\n",
    "            price = items[item].find('span',{'class':\"mp-Listing-price mp-text-price-label\"}).get_text().strip().replace(\"\\xa0\",\" \")\n",
    "            \n",
    "            item_link = 'https://www.2dehands.be/'+ items[item].find('a',{'class':\"mp-Listing-coverLink\"})['href']\n",
    "            dic_new['Title']= title\n",
    "            dic_new['Price']= price\n",
    "            dic_new['item_link']= item_link\n",
    "            q.put(dic_new)\n",
    "            \n",
    "    def open_item(self,item_link):\n",
    "        self.browser.get(item_link)\n",
    "        \n",
    "    \n",
    "            \n",
    "    def scrap_img(self,item_link,item_title):\n",
    "        dic_new={}\n",
    "        dic_new['item_link']= item_link\n",
    "        dic_new['Title']= item_title\n",
    "#         dic_new['Price']= item_price\n",
    "        src = self.browser.page_source\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        imgs = soup.find_all('div',{'class':\"image \"})\n",
    "        for img in range(4):\n",
    "            try:\n",
    "                image = 'https:' + imgs[img].find('img')['src']\n",
    "                dic_new['img_link'+str(img)]= image\n",
    "            except:\n",
    "                pass\n",
    "        price = soup.find('span',{'class':\"price \"}).get_text().strip()\n",
    "        des = soup.find('div',{'class':\"wrapped\"}).get_text().strip()  \n",
    "        dic_new['Price']= price\n",
    "        dic_new['Description']= des[:150]\n",
    "        q.put(dic_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  751  pages found...\n",
      "How many pages to scrap data from?  2\n",
      "Total  751  pages found...\n",
      "How many pages to scrap data from?  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s=Scrap()\n",
    "s.popup()\n",
    "s.search()\n",
    "pages = s.scrap_data()\n",
    "pages = eval(input('How many pages to scrap data from? (excluding main search page)  '))\n",
    "st_page= eval(input('From which page scraping should begin?  '))\n",
    "for page in range(st_page,pages+1):\n",
    "    \n",
    "    s.next_page(page_no = page,search_item='porselein')\n",
    "    s.scrap_data()\n",
    "    time.sleep(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while(not(q.empty())):\n",
    "    count = count + 1\n",
    "    result = q.get()\n",
    "#     print(count,\" --> \" , result)\n",
    "    data_new = data_new.append(result, ignore_index=True)\n",
    "data_new.to_csv(r'2dehands_init.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'2dehands_init.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>item_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Title  Price  item_link\n",
       "0    NaN    NaN          1\n",
       "1    NaN    NaN          2\n",
       "2    NaN    NaN          3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data.iloc[:,3])):\n",
    "    link =data.iloc[i,3]\n",
    "    title =data.iloc[i,1]\n",
    "#     price =data.iloc[i,2]\n",
    "    s.open_item(link)\n",
    "    \n",
    "    s.scrap_img(link,title)\n",
    "    \n",
    "count = 0\n",
    "while(not(q.empty())):\n",
    "    count = count + 1\n",
    "    result = q.get()\n",
    "#     print(count,\" --> \" , result)\n",
    "    data_new1 = data_new1.append(result, ignore_index=True)\n",
    "data_new1.to_csv(r'final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_new1.to_csv(r'2dehands_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser = webdriver.Chrome('chromedriver.exe', options=chrome_options)\n",
    "# a= 'https://www.2dehands.be/v/antiek-en-kunst/antiek-servies-los/m1647240321-antieke-kopjes-en-onderbordjes-morley-ware?c=9b26ed2a557deff636f4f8b9c5b7a618&previousPage=home'\n",
    "# browser.get(a)\n",
    "# src = browser.page_source\n",
    "# soup = BeautifulSoup(src,'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = soup.find_all('div',{'class':\"image \"})\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = 'https:' + imgs[0].find('img')['src']\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# des = soup.find('div',{'class':\"wrapped\"}).get_text().strip()\n",
    "# des[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price = soup.find('span',{'class':\"price \"}).get_text().strip()\n",
    "# price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,time,random,queue\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "data_new= pd.DataFrame(columns= ['Title','Price','item_link'])\n",
    "data_new1= pd.DataFrame(columns= ['Title','Price','Description','item_link'])\n",
    "q = queue.Queue()\n",
    "def clear(): \n",
    "  \n",
    "    # for windows \n",
    "    if name == 'nt': \n",
    "        _ = system('cls') \n",
    "  \n",
    "    # for mac and linux(here, os.name is 'posix') \n",
    "    else: \n",
    "        _ = system('clear') \n",
    "chrome_options = Options()\n",
    "\n",
    "############################################################\n",
    "class Scrap:\n",
    "    \n",
    "\n",
    "    def popup(self):\n",
    "        self.browser = webdriver.Chrome('chromedriver.exe', options=chrome_options)\n",
    "        self.browser.get('https://www.2dehands.be/')\n",
    "        time.sleep(10)\n",
    "        self.browser.find_element_by_id(\"gdpr-consent-banner-accept-button\").click()\n",
    "        \n",
    "    def search(self,search_item='porselein'):\n",
    "        search_200 = 'https://www.2dehands.be/q/{s}/'.format(s=search_item)\n",
    "        self.browser.get(search_200)\n",
    "        time.sleep(2)\n",
    "    def next_page(self,page_no,search_item='porselein'):\n",
    "        next_page ='https://www.2dehands.be/q/{s}/p/{p}/'.format(s=search_item,p=page_no)\n",
    "        self.browser.get(next_page)\n",
    "        try:\n",
    "            browser.find_element_by_class_name('hz-Icon hz-Icon--m hz-SvgIcon hz-SvgIconClose').click()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "    def scrap_data(self):\n",
    "        dic_new={}\n",
    "        src = self.browser.page_source\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        total_pages= soup.find('span',{'class':\"mp-PaginationControls-pagination-pageList\"}).find_all('span')[2].get_text()\n",
    "        print('Total ',total_pages,' pages found...')\n",
    "        \n",
    "        items = soup.find_all('li',{'class':\"mp-Listing mp-Listing--list-item \"})\n",
    "        time.sleep(3)\n",
    "        for item in range(33):\n",
    "            dic_new={}\n",
    "            title = items[item].find('h3',{'class':\"mp-Listing-title\"}).get_text().strip()\n",
    "            price = items[item].find('span',{'class':\"mp-Listing-price mp-text-price-label\"}).get_text().strip().replace(\"\\xa0\",\" \")\n",
    "            \n",
    "            item_link = 'https://www.2dehands.be/'+ items[item].find('a',{'class':\"mp-Listing-coverLink\"})['href']\n",
    "            dic_new['Title']= title\n",
    "            dic_new['Price']= price\n",
    "            dic_new['item_link']= item_link\n",
    "            q.put(dic_new)\n",
    "            \n",
    "    def open_item(self,item_link):\n",
    "        self.browser.get(item_link)\n",
    "        \n",
    "    \n",
    "            \n",
    "    def scrap_img(self,item_link,item_title):\n",
    "        dic_new={}\n",
    "        dic_new['item_link']= item_link\n",
    "        dic_new['Title']= item_title\n",
    "#         dic_new['Price']= item_price\n",
    "        src = self.browser.page_source\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        imgs = soup.find_all('div',{'class':\"image \"})\n",
    "        for img in range(4):\n",
    "            try:\n",
    "                image = 'https:' + imgs[img].find('img')['src']\n",
    "                dic_new['img_link'+str(img)]= image\n",
    "            except:\n",
    "                pass\n",
    "        price = soup.find('span',{'class':\"price \"}).get_text().strip()\n",
    "        des = soup.find('div',{'class':\"wrapped\"}).get_text().strip()  \n",
    "        dic_new['Price']= price\n",
    "        dic_new['Description']= des[:150]\n",
    "        q.put(dic_new)\n",
    "        \n",
    "\n",
    "s=Scrap()\n",
    "s.popup()\n",
    "s.search()\n",
    "pages = s.scrap_data()\n",
    "pages = eval(input('How many pages to scrap data from? (excluding main search page)  '))\n",
    "st_page= eval(input('From which page scraping should begin?  '))\n",
    "for page in range(st_page,pages+1):\n",
    "    \n",
    "    s.next_page(page_no = page,search_item='porselein')\n",
    "    s.scrap_data()\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "count = 0\n",
    "while(not(q.empty())):\n",
    "    count = count + 1\n",
    "    result = q.get()\n",
    "#     print(count,\" --> \" , result)\n",
    "    data_new = data_new.append(result, ignore_index=True)\n",
    "data_new.to_csv(r'2dehands_init.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <img class=\"Carousel-image false\" src=\"//i.ebayimg.com/00/s/MTAyNFg3Njg=/z/0ecAAOSw-e1f7Z-M/$_84.JPG\" alt=\"Antieke kopjes en onderbordjes morley ware\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
